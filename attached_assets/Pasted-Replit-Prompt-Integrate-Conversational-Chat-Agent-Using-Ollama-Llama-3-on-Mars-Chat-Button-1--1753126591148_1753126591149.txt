Replit Prompt: Integrate Conversational Chat Agent Using Ollama (Llama 3) on Mars² Chat Button
✅ 1. YOUR SETUP INCLUDES:
Frontend: React UI (Mars² dApp)

AI Engine: Llama 3 running locally via Ollama

Backend: Express.js server that:

Fetches real-time validator events & data

Reads Mars² documentation

Fuses all into context and passes to the AI agent

🔌 2. Setting Up the Local AI Engine (Ollama)
A. Install Ollama
Instructions: https://ollama.com

bash
# Install
curl -fsSL https://ollama.com/install.sh | sh
# or use brew (macOS)
brew install ollama

# Pull Llama 3
ollama pull llama3
🧠 You now have a local endpoint at:
http://localhost:11434/v1/chat/completions

Test it:

bash
curl http://localhost:11434/api/chat -d '{"model":"llama3","messages":[{"role":"user","content":"What is staking on Sei?"}]}'
🛠 3. Backend (Node/Express): Serve AI Endpoint
Create server.js:

js
import express from "express";
import fetch from "node-fetch";
import fs from "fs";
import { ethers } from "ethers";

const app = express();
app.use(express.json());

// Load Mars² docs
const mars2Docs = fs.readFileSync('./docs/README.md', 'utf-8');

// Mars² smart contract (Sei)
const provider = new ethers.JsonRpcProvider("https://evm-rpc.testnet.sei.io");
const contract = new ethers.Contract(
  "0x2358F2a3A43aa1D43A1B6A04D52E26b7c37B294",
  ["function getEvents(address) view returns (tuple(string label,int256 delta,uint timestamp)[])"],
  provider
);

// API endpoint for chat agent
app.post("/api/ai/chat", async (req, res) => {
  const { userQuestion } = req.body;

  // Get validator data (optional demo)
  const validators = await fetch("https://sei.explorers.guru/api/validators")
    .then(r => r.json())
    .catch(() => []);

  const recentValidatorExample = validators?.[0] || {};
  const validatorInfoSummary = `Sample validator: ${recentValidatorExample?.moniker}, status: ${recentValidatorExample?.status}, uptime: ${recentValidatorExample?.uptime}`

  // Inject docs + validator context
  const messages = [
    {
      role: "system",
      content: `
You are Mars², an expert staking assistant for the Sei blockchain.
You answer questions about staking, slashing, validator behavior, and how Mars² enhances security via risk scoring and encryption.
Use exact language and reference validator incidents where relevant.
Here is context:
--- Mars² Documentation ---
${mars2Docs}

--- Recent Validator Info ---
${validatorInfoSummary}
      `
    },
    {
      role: "user",
      content: userQuestion,
    },
  ];

  const aiRes = await fetch("http://localhost:11434/v1/chat/completions", {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify({
      model: "llama3",
      messages,
      stream: false
    }),
  });

  const data = await aiRes.json();
  const content = data.choices[0].message.content;
  res.json({ reply: content });
});

app.listen(3001, () => {
  console.log("✅ Mars² AI chat server running on http://localhost:3001");
});
💬 4. Frontend: Create Sidebar Chat + “Chat” Button Component
A. Add SidebarChat.jsx
jsx
import React, { useState } from "react";

export default function SidebarChat() {
  const [messages, setMessages] = useState([]);
  const [input, setInput] = useState("");
  const [loading, setLoading] = useState(false);

  const sendMessage = async () => {
    const currentMessage = input;
    const newHistory = [...messages, { role: "user", content: currentMessage }];
    setMessages(newHistory);
    setInput("");
    setLoading(true);

    const res = await fetch("/api/ai/chat", {
      method: "POST",
      headers: { "Content-Type": "application/json" },
      body: JSON.stringify({ userQuestion: currentMessage }),
    });
    const data = await res.json();
    const newMessages = [...newHistory, { role: "assistant", content: data.reply }];
    setMessages(newMessages);
    setLoading(false);
  };

  return (
    <div className="chat-container">
      <h3>🔍 Ask Mars²</h3>
      <div className="chat-history">
        {messages.map((msg, i) => (
          <div key={i} className={`msg ${msg.role}`}>{msg.content}</div>
        ))}
        {loading && <div className="msg assistant">Thinking...</div>}
      </div>
      <div className="chat-input">
        <input
          value={input}
          onChange={(e) => setInput(e.target.value)}
          placeholder="Ask about staking, validators, risks..."
        />
        <button onClick={sendMessage}>Send</button>
      </div>
    </div>
  );
}
B. Add Chat Toggle Button in App
jsx
import SidebarChat from './components/SidebarChat';

function App() {
  const [chatOpen, setChatOpen] = useState(false);

  return (
    <div className="App">
      <button onClick={() => setChatOpen(!chatOpen)}>💬 Chat</button>
      {chatOpen && <SidebarChat />}
      {/* rest of Mars² page */}
    </div>
  );
}
C. Quick CSS
css
.chat-container {
  position: fixed;
  right: 1rem;
  bottom: 1rem;
  width: 300px;
  max-height: 80%;
  background: #1a1a1a;
  color: white;
  padding: 12px;
  border-radius: 8px;
  overflow: auto;
}
.chat-history {
  max-height: 300px;
  overflow-y: scroll;
  margin-bottom: 10px;
}
.msg.user {
  color: cyan;
  text-align: right;
}
.msg.assistant {
  color: white;
  text-align: left;
}
.chat-input {
  display: flex;
  gap: 6px;
}
.chat-input input {
  flex: 1;
  padding: 6px;
  background: #111;
  color: white;
  border: 1px solid #444;
}
.chat-input button {
  padding: 6px 10px;
}
🧪 5. Test Your Full Stack on Replit
☑ Ollama is running (ollama serve)
☑ You pulled Llama 3 (ollama pull llama3)
☑ Your backend server is running on port 3001
☑ You test this prompt:

jsx
"What happened to RHINO validator?"  
"What is staking on Sei?"  
"How does Mars2 prevent validator risk?"  
"Explain slashing risk on Sei."  
"What’s the safest validator right now?"  
✅ Recap: What This Adds
Seamless Chat Button triggers a full UI+Agent sidebar

All questions are routed to Llama 3 via Ollama

Real Sei validator data and Mars² docs are passed as system context

Answers include: validator risk, staking info, Mars² zk-reporting, encrypted messages, scores

Multi-turn logic is optional and ready